{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c7a1b11-b009-4390-be3a-f5de92b73285",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fa9a4d-ffd2-4328-8b47-eee3c93c82a1",
   "metadata": {},
   "source": [
    "A regression model helps us understand the relationship between input variables(or independent variables) and output variables(or dependent variables. There types of regression techniques like linear regression,logistic regression etc.\n",
    "    \n",
    "A linear regression model is used if the relationship between the dependent and independent variables is expected to be linear. By applying this model,we get a best fit line which predicts approximate output varible for given input variable(s).\n",
    "                                                                                                                                                                                                                       \n",
    "A logistic regression model is used when the output is binary,that is,either 0 or 1,either True or False etc. It is used when the output can either this or that. While a linear regression model tries ti find a best fit linear relation(that is, a line),the logistic regression model fits the data on a logistic sigmoid function.\n",
    "\n",
    "A sigmoid function is an S-shaped monotonic differentiable function that has bounds,usually but not neccesarily bounded between 0 and 1. Some examples of sigmoid function are logistic sigmoid,hyperbolic tangent function etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61d3636-2421-44c2-8689-a070ac3e92ab",
   "metadata": {},
   "source": [
    "The logistic sigmoid function is defined as:\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de9f6b7-b124-49f0-a807-0a28a93206e9",
   "metadata": {},
   "source": [
    "The hyperbolic tangent function is defined as:\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56cc523-0b4c-4ba5-afa8-37ff0f31fc5f",
   "metadata": {},
   "source": [
    "The logistic regression model uses the logistic sigmoid function (also called logit function). For a dataset containing 2 independent variables,the logt function can be modified as "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d546554e-bbd1-4f58-bbde-9e2fe9918d44",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma(x_1,x_2) = \\frac{1}{1 + e^{-(\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83280ce9-0da2-4293-b7e0-0cb771ee802b",
   "metadata": {},
   "source": [
    "where $x_1$ and $x_2$ are the independent variables and $\\theta_0$ , $\\theta_1$ and $\\theta_2$ are the parameters that we have to find. Once these parameters are found,we can use them to find the value of the logit function for given values of independent variables. This value is nothing but the probability that the event will happen for the given values of independent variables. But logistic regression is a classification model,that is,can have only 2 outputs which are 0 or 1. Therefore we need a threshold value,if the probability is above this value then the output will be 1 and 0 if the probability is below it. The threshold value that we will be using is 0.5 ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09464fb-7dc4-462b-8bba-b2a3066b4ed3",
   "metadata": {},
   "source": [
    "But how do we find the correct parameter values? To find the correct parameters we need to know about cost function and gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c6ec04-b947-4f4e-8b2d-92551aaf3c4f",
   "metadata": {},
   "source": [
    "### Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce249f03-90d3-41d7-aa50-c36d701f4ae4",
   "metadata": {},
   "source": [
    "In general,cost function is a function whose value at given vectors or scalars is a scalar which is a measure of how well the model predicts. So,a low value of this cost function means that the accuracy of this model is high. There are many types of cost functions,for example mean squared error,mean absolute error etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fb3427-9333-4dc8-84b5-f745cc41702c",
   "metadata": {},
   "source": [
    "The cost function that we use for logistic regression model is\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(h_\\theta(x_i)) + (1 - y_i) \\log(1 - h_\\theta(x_i)) \\right]   \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7ac798-0cf7-49db-bead-868d2dd92996",
   "metadata": {},
   "source": [
    "where n is the data size , $y_i$ is the actual output(either 0 or 1) at x=$x_i$ , $h_\\theta(x_i)$ is the predicted value of the output  at x=$x_i$ . Note that $x_i$ is the vector representation of the independent variables and $\\theta$ is the vector representation of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a861c146-11d9-4682-851d-2dcbbfaf5439",
   "metadata": {},
   "source": [
    "The main purpose of cost function is its helpfulness in increasing the accuracy of the model.To maximise the accuracy,we minimise the cost function and this can be done using various algorithms. The one we are going to use is gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ffc63e-c2e1-4422-8a36-f0bedbf893e2",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c00372-3548-429c-b8e5-6ed6144123e1",
   "metadata": {},
   "source": [
    "Consider the function $f(x)=x^2+2x+1$ . The point of minima for any function has a gradient of 0. Using this fact,one can directly find the minima by differentiating the quadratic polynomial and equate it to 0. But it can also be solved in another method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d817ae2e-f55f-45e1-af61-f571e360dd04",
   "metadata": {},
   "source": [
    "We know that gradient points towards the direction with steepest increase of the function and its value represents how fast the function rises. This means that the direction exactly opposite to the gradient direction is direction of steepest decrease and the value of the gradient represents how fast the function drops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df63a00-2d30-45df-9077-84d925deb101",
   "metadata": {},
   "source": [
    "So,if we select a random point on the curve,move in a direction opposite to the gradient vector and move by a distance propotional to the value of the gradient,the new position we reach will be closer to the point of minima.If this process is repeated enough number of times with a correct choice of propotionality constant,we will reach a point which is so close to the point minima that for all practical applications this point can be considered as the point of minima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb89ed0-4d2b-49ca-b323-a530fd92134f",
   "metadata": {},
   "source": [
    "The proportionality constant here is called learning rate.It is a hyperparameter and hence must be given by the user (but not calculated by the machine) before training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fdff6f-4dd0-40d2-a7ea-ea3596b83799",
   "metadata": {},
   "source": [
    "Let us test this technique on $f(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d8bd134-8604-401e-9f80-81daf4ae7121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.34464000000000006"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=1\n",
    "def gradient(x):\n",
    "    return 2*x+2\n",
    "\n",
    "learning_rate=0.1\n",
    "number_of_iterations=5\n",
    "def gradient_descent(x,number_of_iterations,learning_rate):\n",
    " for i in range(number_of_iterations):\n",
    "     x=x-gradient(x)*learning_rate\n",
    " return x\n",
    "xn=gradient_descent(x,number_of_iterations,learning_rate)\n",
    "xn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3e4233-8e01-4d09-b739-788024d4e220",
   "metadata": {},
   "source": [
    "For learning rate=0.1 and number of iterations=5,we got -0.34. Let us try again with different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30627cbc-a88f-44d7-b005-0ef6753c5619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.84448"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate=0.2\n",
    "number_of_iterations=5\n",
    "xn=gradient_descent(x,number_of_iterations,learning_rate)\n",
    "xn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a711867-305a-43cc-8a1a-d34a6f01b8e9",
   "metadata": {},
   "source": [
    "By increasing learning rate to 0.2,we got -0.84 which is a better estimation as required answer is -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6a83a6f-f298-4bd0-8fb3-d6696c29d27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7852516352000001"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate=0.1\n",
    "number_of_iterations=10\n",
    "xn=gradient_descent(x,number_of_iterations,learning_rate)\n",
    "xn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23233cbe-c229-4a9e-b678-c3ae5f70cce0",
   "metadata": {},
   "source": [
    "Increasing the number of iterations has also helped in getting a better estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2f512e4-cf11-43b5-be23-75f81a508bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.15552"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate=0.8\n",
    "number_of_iterations=5\n",
    "xn=gradient_descent(x,number_of_iterations,learning_rate)\n",
    "xn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fea1b0-7c1f-40f0-816f-5aa0609726ae",
   "metadata": {},
   "source": [
    "But for a higher value of learning rate,we missed the point of minima,that is,we have travelled too much distance that we crossed the point of minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b133414a-43cd-4c1c-b691-05394f76386e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9999999999838344"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate=0.8\n",
    "number_of_iterations=50\n",
    "xn=gradient_descent(x,number_of_iterations,learning_rate)\n",
    "xn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb925c4-0aee-4e73-9ffe-2b33c2559821",
   "metadata": {},
   "source": [
    "But setting the number of iterations to a higher value has brought us back, very near to the point of minima.So,can we set the learning rate to any value and set the number of iterations to a very high value? It may work but it isn't suggested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1207fbd7-f8b0-47a3-afd7-dadbc696b34c",
   "metadata": {},
   "source": [
    "In our case increasing the number of iterations from 5 to 50 seemed like it didn't effect the time of computation because 5 and 50 are actually a small number of iterations and the calculation itself is basic. But in real life applications,the calculation is a lot more bulkier and the number of iterations is much much higher.On top of that,if we set the learning rate randomly and increase the number of iterations to a much more higher value,the time of computation becomes too large. So even though it may work,it is suggested not to set the learning rate randomly but make some calculations if possible or otherwise make an intelligent guess."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489362aa-7e66-4d3d-835a-b1b57b33a278",
   "metadata": {},
   "source": [
    "To find the best learning rate,there are techniques like grid search,random search etc. We shall use the random search technique,where we first make a prediction of the range in which the learning rate may lie,then iterate through as many values of learning rates as possible to find the best one in the assumed range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c167868-a47e-4c70-98ef-549739554978",
   "metadata": {},
   "source": [
    "Though this method seems to be much bulkier than the first method for the quadratic polynomial,this method is still useful. In this case we know the function and it a 2 dimensional curve. But for higher dimensional functions,this method is much more helpful. So we will be using this method to minimise our cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36681f91-5f08-4d95-af14-864202b32092",
   "metadata": {},
   "source": [
    "The gradient of the logit function is \n",
    "$$\n",
    "\\nabla J(\\theta)=\\frac{1}{n} (X^T).(h-y)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eeb190-6c90-46af-bec0-6467f91cee7a",
   "metadata": {},
   "source": [
    "where X is the vector(that is,a matrix) containing the independent variables of all the data points in the data set,h is the vecotr containing all the predicted values of the data points and y is the vector containing all the correct values of the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f240ef-4274-4417-8b01-0f5d5c883aa3",
   "metadata": {},
   "source": [
    "With this knowledge,let us now start writing the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aedaf52-e88a-4b00-840d-2fdbd6a369ff",
   "metadata": {},
   "source": [
    "### The Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22055cf-b0cf-474a-b62b-a7e7cce89355",
   "metadata": {},
   "source": [
    "Importing the neccessary libraries and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adc373d7-3ae5-4288-9320-38cd5db184df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_train=pd.read_csv(r\"D:\\data2_train.csv\")\n",
    "df_test=pd.read_csv(r\"D:\\data2_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d0676a-e563-4091-b836-aba847099da8",
   "metadata": {},
   "source": [
    "Now,we initiate the $\\theta$ vector . In this vector , first co-ordinate will be the constant term $\\theta_0$ and the next two are the coefficients $\\theta_1$ and $\\theta_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "987543ed-4ef9-4492-8c0a-ac1b61cc01e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta=np.ones((3,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f934e745-1f3a-4c62-94e1-44277cd476cf",
   "metadata": {},
   "source": [
    "For the logit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1595a7f-b184-4e2f-88bf-913bc42645f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logitfunc(x,t):\n",
    "  c=np.exp(np.dot(x,t))\n",
    "  return c/(1+c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65074c2-9626-44e8-85a2-361e94ab50da",
   "metadata": {},
   "source": [
    "Let us now prep the vectors needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "658519e0-f000-48c9-9afc-7a47cd095588",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df_train.iloc[:,:2]\n",
    "y=df_train.iloc[:,2:3]\n",
    "x=np.insert(x,0,1,axis=1)\n",
    "n=len(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45919cb9-dc25-4f91-92d4-ab16f0f97253",
   "metadata": {},
   "source": [
    "Here we have added a column containing ones.These ones will not be changed and are added to match the dimensions of $\\theta$ as we have to find the dot product of these vectors.So when they multiplied , we get $\\theta_0$ + $\\theta_1$$x_1$ + $\\theta_2$$x_2$ , which is the expression that appears in the expression of logit function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c22885-6285-4fa0-ade6-835ba4a9079b",
   "metadata": {},
   "source": [
    "Let us also define the gradient of the logit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "105ea784-73b4-40db-8a0b-0680d856ce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientfunc(x,y,n):\n",
    "   h=logitfunc(x,theta)\n",
    "   return (1/n)*np.dot((x.T),(h-y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a348f8-859a-4a7e-bd86-d78032299c65",
   "metadata": {},
   "source": [
    "For updating the $\\theta$ vector in an iterative manner,let us write another function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e8a0697-1821-44d3-8c4d-a7ac23a03624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientoptim(x,y,n,theta,lr,num_iter):\n",
    " for i in range(num_iter):\n",
    "   g=gradientfunc(x,y,n)\n",
    "   theta-=g*lr\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbad168-d9da-490e-bbb6-26f40773a6eb",
   "metadata": {},
   "source": [
    "Let us now set the number of iterations and learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f58e0f3-c88b-4461-a5ab-d63fa691f489",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter=10000\n",
    "lro=0.31\n",
    "n=len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0246bc43-793a-47c1-8be4-665939ffa559",
   "metadata": {},
   "source": [
    "Let us now perform the gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3f03884-84fe-425c-9c6e-ecc7101fc17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientoptim(x,y,n,theta,lro,num_iter)\n",
    "h=logitfunc(x,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aaf20cd5-7d44-4044-b38e-8d55d07dd89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.89334441e-31]\n",
      "[3.31214771e-37]\n",
      "[2.31392988e-33]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(h[i])\n",
    "\n",
    "for i in range(3):\n",
    "    print(h[799-i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b441a0f9-7eb4-408d-9d87-a9ae40c80436",
   "metadata": {},
   "source": [
    "Clearly,the values at the start (which are supposed to be 0) are very near to zero and the values at the end (which are supposed to be 1) are very near to one. This means the model is performing,let us check its accuracy on the training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14148142-e7fc-433f-9aac-dc492856bd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on training data is :99.125%\n"
     ]
    }
   ],
   "source": [
    "def setnormal(h):\n",
    " for i in range(len(h)):\n",
    "   if h[i]<0.5:\n",
    "      h[i]=0\n",
    "   else:\n",
    "      h[i]=1\n",
    "setnormal(h)\n",
    "def accuracyfunc(h,y):\n",
    " count=(h==y).sum()\n",
    " accuracy=(count.values[0]/n)*100\n",
    " return accuracy\n",
    "print(f\"The accuracy on training data is :{accuracyfunc(h,y)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81792a18-4b54-4556-bcaf-0f74b7808740",
   "metadata": {},
   "source": [
    "But is this the maximum that can be attained? Let random search on learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6855c7a4-b283-4041-9e7e-e13d4fc0d6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(x,y,theta,iter,rangelo,rangehi):\n",
    "    hi_lr=0\n",
    "    hi_accuracy=0\n",
    "    for i in range(iter):\n",
    "        lrn=np.random.uniform(rangelo,rangehi)\n",
    "        theta_c=theta.copy()\n",
    "        gradientoptim(x,y,n,theta_c,lrn,num_iter)\n",
    "        hn=logitfunc(x,theta)\n",
    "        setnormal(hn)\n",
    "        accuracy_n=accuracyfunc(hn,y)\n",
    "        if accuracy_n>hi_accuracy:\n",
    "            hi_accuracy=accuracy_n\n",
    "            hi_lr=lrn\n",
    "    return hi_lr,hi_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "826504dc-0171-49a0-81df-50f603a03c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate=0.2779272825673684 and Accuracy=99.125\n"
     ]
    }
   ],
   "source": [
    "p=random_search(x,y,theta,10,0,1)\n",
    "print(f\"Learning rate={p[0]} and Accuracy={p[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07eff57c-9786-4b2d-bb93-a09ab34d557b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on testing data:98.5%\n"
     ]
    }
   ],
   "source": [
    "x2=df_test.iloc[:,:2]\n",
    "y2=df_test.iloc[:,2:3]\n",
    "x2=np.insert(x2,0,1,axis=1)\n",
    "n=len(df_test)\n",
    "h2=logitfunc(x2,theta)\n",
    "setnormal(h2)\n",
    "acc_test=accuracyfunc(h2,y2)\n",
    "print(f\"Accuracy on testing data:{acc_test}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b40ee54-9edc-48f1-8a5e-98c9f4764872",
   "metadata": {},
   "source": [
    "Let us compare this with the accuracy of the model trained by scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd63deed-9aa0-49cc-a58e-352edbae5297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training dataset: 98.625%\n",
      "Accuracy on testing dataset: 99.0%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "X_train = df_train[['Feature_1', 'Feature_2']]\n",
    "y_train = df_train['Target']\n",
    "\n",
    "X_test = df_test[['Feature_1', 'Feature_2']]\n",
    "y_test = df_test['Target']\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred)*100\n",
    "print(f\"Accuracy on training dataset: {accuracy_train}%\")\n",
    "\n",
    "y_test_pred = model.predict(X_test)\n",
    "accuracy_test = accuracy_score(y_test, y_test_pred)*100\n",
    "print(f\"Accuracy on testing dataset: {accuracy_test}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
